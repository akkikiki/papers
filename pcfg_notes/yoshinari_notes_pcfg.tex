\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsbsy}
\usepackage{url}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}

\section{Summary of what I did understand (or misunderstand)}
Objective: We want to know $\theta$, a probability distribution over rewrite rules in PCFG, given the observed sentences $\boldsymbol{s}$ i.e. 
$$
P(\theta|\boldsymbol{s})
$$


Apply Bayesian framework:
$$
P(\theta|\boldsymbol{s}) \propto P(\boldsymbol{s}|\theta) P(\theta)
$$

Use Dirichlet prior on each $\theta_A$ (where $A$ is a non-terminal):
$$
\theta_A \sim Dirichlet(\alpha_A) 
$$

As a result of including Dirichlet prior, $\theta$ now depends on a hyper-parameter $\alpha$ of a Dirichlet distribution. Thus, the posterior distribution $P(\theta|\boldsymbol{s})$ can be rewritten as \cite{cohen-johnson:2013:ACL2013}:

$$
P(\theta|\boldsymbol{s}, \alpha) \propto P(\boldsymbol{s}|\theta) P(\theta | \alpha)
$$


Assume that $s$ is distributed as $Multinomial$ i.e. $\boldsymbol{s} \sim Multinomial(\theta)$. Then, the posterior $P(\theta|\boldsymbol{s})$ or $P(\theta|\boldsymbol{s}, \alpha)$ is also distributed as $Dirichlet$:
$$
\theta|\boldsymbol{s} \sim Dirichlet(f_{\boldsymbol{t}} + \alpha)
$$
where $f_{\boldsymbol{t}}$ is the vector of production counts in $\boldsymbol{t}$ indexed by $r \in R$ \cite{johnson-griffiths-goldwater:2007:main}.

\section{Training Bayesian PCFG under supervised and unsupervised setting}

Under supervised setting, since the data consists of parse trees $\boldsymbol{t}$ \cite{cohen-johnson:2013:ACL2013}, simply replace $\boldsymbol{s}$ by $\boldsymbol{t}$:
$$
P(\boldsymbol{s}|\theta) =  P(\boldsymbol{t}|\theta) = \prod_i P(t_i | \theta)
$$

Under unsupervised setting, since the data consists of sentences $\boldsymbol{s}$,
$$
P(\boldsymbol{s}|\theta) = \prod_i P(s_i | \theta) = \prod_i \sum_{t_i \in T: yield(t_i) = s_i} P(t_i | \theta)
$$

% TODO: not fully understood this part yet
%Moreover, the likelihood function becomes
%$$
%P(\boldsymbol{s}|\alpha) = \sum_t P(\boldsymbol{s}, t|\alpha) = \sum_t \int P(\boldsymbol{s}, t|\boldsymbol{\theta}) P(\boldsymbol{\theta}|\alpha) d\theta
%$$



Moreover, in unsupervised setting, we are also interested in knowing the parse trees $t$ for given sentences $s$. So the posterior $P(\theta|\boldsymbol{s}, \alpha)$ becomes:
$$
P(\theta|\boldsymbol{s}, \alpha) = \sum_{\boldsymbol{t}} P(\boldsymbol{t}, \theta | \boldsymbol{s}, \alpha) 
$$

In conclusion, under both unsupervised setting with Bayesian framework, our objective is to compute the posterior $P(\boldsymbol{t}, \theta | \boldsymbol{s}, \alpha)$ using Dirichlet prior with hyper-parameter $\alpha$.

\section{Marginal Likelihood vs. Joint likelihood}

Once again, $\theta \sim Dirichlet(\alpha)$. 
In general, marginal likelihood is a likelihood function in which some parameter variables are marginalized out. Note that marginal likelihood


when a variable $\theta$ is marginalized out.

%With labeled data $D = \{x_i, y_i\}$, the joint likelihood of input and output \cite{joint_vs_marginal} is
%$$
%\argmax_{\theta} L(\theta|X, Y) = \argmax_{\theta} \prod^n_{i = 1} P(y_i|\theta) P(x_i|y_i, \theta).
%$$
%
%With unlabeled data $D = \{x_i\}$, the marginal likelihood of input \cite{joint_vs_marginal} is
$$
\argmax_{\theta} L(\theta|X) = P(X|\theta) = \argmax_{\theta} \prod^n_{i = 1} \sum_{y \in \Omega_Y}P(y|\theta) P(x_i|y, \theta).
$$
%which need to compute every way of filling in the missing labels $y$ \cite{mlss09}.
%
%
%Marginal likelihood is used when training data is unlabeled \cite{joint_vs_marginal} (Cited slides use HMM as an example). 
%With unlabeled training data, we maximized the marginal likelihood of the input. In HMM case, the parameters $\boldsymbol{\theta}$ are transition probabilities $A$, emission probabilities $\phi$, and probability of being an initial state $\pi$ \cite{hmm_parameters}. In \cite{joint_vs_marginal}, the parameters $\boldsymbol{\theta}$ is a bit unclear so let me clarify. Transition probabilities can be computed using state sequence $\boldsymbol{y}$ alone. Therefore, $P(y_i|\theta)$ or $P(y_i|A, \pi)$ is the transition probability. Emission probabilities can be computed the observed sequence and the state sequence $\boldsymbol{x}$ and $\boldsymbol{x}$. Therefore, $P(x_i|y_i,\theta)$ or $P(x_i|y_i, \phi)$ is the emission probability. 

\subsection{Why Marginal Likelihood?}
Since outputs (= labels) are not given in latent variable models \cite{liang_jordan_klein}, we cannot compute the joint likelihood of inputs and outputs. Therefore, we would like to compute the marginal likelihood (by summing over all outputs for a given input) \cite{pcfg_marginal}. If we are using EM to learn the rule probabilities, then it attempts to maximize the marginal likehood of inputs \cite{pcfg_marginal}.

Note that marginal likelihood can appear in either Bayesian framework or non-Bayesian framework.

\section{Summary of Parameter Learning \cite{mlss09}}

Here, we assume that the prior is dirichlet distribution, likelihood is multinomial distribution, and therefore, the posterior distribution is also Dirichlet distribution.

\begin{center}
    \begin{tabular}{|l|l|l|}
    \hline
             & labeled data                    & unlabeled data  \\\hline
    ML       & frequency                       & EM              \\\hline
    Bayesian & updated Dirichlet distribution  & MCMC/VB         \\\hline
    \end{tabular}
\end{center}

\section{Key Features}
\begin{enumerate}
 \item PCFG
 \item Bayesian Framework
 \item Variational Bayes, Gibbs sampling, particle filter
 \item Unsupervised (parse trees not observed)
\end{enumerate}

\section{Misc. Notes}
In a supervised setting, a tree $t$ and a sentence $s$ are both observable.


\section{Future Studies}
\begin{enumerate}
 \item Read \cite{beal2003variational} and \cite{kurihara2004application}.
 \item Variational Inference, Gibbs sampling, and particle filter in general.
 \item Start from assuming that the parse trees $\boldsymbol{t}$ are observed. Then, study about the Bayesian framework applied to PCFG.
 \item Review the NLP lectures by Chris Manning on Coursera
 \item What is marginal likelihood?
\end{enumerate}




\bibliographystyle{plain}
\bibliography{yoshinari_notes_pcfg}

\end{document}

