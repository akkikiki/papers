\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{url}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}

\section{Summary}

This is a note for \cite{bhatia-guthrie-eisenstein:2016:EMNLP2016}.
The implementation code for this paper can be found at \footnote{\url{https://github.com/rguthrie3/MorphologicalPriorsForWordEmbeddings}}.

The important components are as follows:
\begin{enumerate}
 \item RNN language model
 \item Morphological priors
 \item Latent word embedding $b_w$.
 \item Morpheme emebedding $u_m$.
 \item Variational distribution $Q(b)$
\end{enumerate}

\section{Latent Word Embedding and Morpheme Embedding}
Each morpheme is segmented in unsupervised fashion according to Morfessor. For example, $u_{-ism} = (-0.24, 5, -111)$.

When inferring $P(x)$, we will have to infer $P(b)$ too since $P(b)$ appears in the lower variational bound.

% Aditiya: You have to write it out. There is no other way or other shortcuts.


$$
b_{w,i} \sim Bernouli(sigmoid(\sum_{m \in M_w} u_{m,i}))
$$
i.e. for outcomes or the range of a probabilistic variable $b_{w,i}$ is either $0$ or $1$,

$$
P(b_{w,i}) = sigmoid(\sum_{m \in M_w} u_{m,i})^{b_{w,i}} (1 - sigmoid(\sum_{m \in M_w} u_{m,i}))^{1 - b_{w,i}}
$$

So let's look into an example. 
Let $M = {perfection, -ism}$
$u_{perfection} = (0, -1.1, 1)$ 

$u_{-ism} = (2, 5.1, 3)$ 

When $w = perfectionism$, then

$b_{w,0} \sim Bernoulli(sigmoid(0 + 2)) \approx 0.88$

$b_{w,1} \sim Bernoulli(sigmoid(-1.1 + 5.1)) \approx 0.98$

$b_{w,2} \sim Bernoulli(sigmoid(1 + 3)) \approx 0.98$

So $P(b_w = (1, 1, 1)) = 0.88 * 0.98 * 0.98 \approx 0.84$.

\section{Hidden state}
The hidden state at time $h_t$ (vector) is
$$
h_t = sigmoid(\Theta h_{t-1} + b_{x_t})
$$
where $x_t$ is the word corresponding to the position $t$, and $\Theta$ is the parameter for the recurrence function (recurrent weights\footnote{\url{http://peterroelants.github.io/posts/rnn_implementation_part01/}}).

\section{What is going on inside $D_{KL}(Q(b)||P(b))$?}

$$
D_{KL}(q(b_{w,i}) || P(b_{w,i})) = q(b_{w,i}) \log(\frac{q(b_{w,i})}{P(b_{w,i})})
$$

$$
= q(b_{w,i}) (\log(q(b_{w,i})) - \log(P(b_{w,i}))
$$

$$
= q(b_{w,i}) (\log(q(b_{w,i})) - b_{w,i} \log(sigmoid(\sum_{m \in M_w} u_{m,i})) - (1 - b_{w,i}) \log(1 - sigmoid(\sum_{m \in M_w} u_{m,i})))
$$


$$
q(b_{w,i};\gamma_{w,i}) = \gamma_{w,i}^{b_{w,i}} (1 - \gamma_{w,i})^{1 - b_{w,i}}
$$

$$
 \log(q(b_{w,i};\gamma_{w,i})) = b_{w,i}\gamma_{w,i} + (1 - b_{w,i})(1 - \gamma_{w,i})
$$





\bibliographystyle{plain}
\bibliography{yoshinari_notes_prob_neural_word_embed}

\end{document}

