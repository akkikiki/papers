# Machine Teaching
[Curriculum Learning, ICML 2009](http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)
* Adding one more variable to the cost function which is easier to optimize than the other.

[Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education, AAAI 2015](http://pages.cs.wisc.edu/~jerryzhu/pub/MachineTeachingAAAI15.pdf)
* Nice introduction to optimizing the training data for a given parameter optimization technique.

# Tweets
[Shared common ground influences information density in microblog texts, NAACL 2015](http://langcog.stanford.edu/papers/DF-underreview.pdf)
* Using external data for baseball is a good idea. Analysis is through and deep from multiple perspectives.

[The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter, ACL2014](http://www.aclweb.org/anthology/P/P14/P14-1017.pdf)
* Related to the characteristics of language used on Twitter

# Finite State Transducers / Finite State Automata
[Direct Construction of Minimal Acyclic Subsequential Transducers, 2001](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698)
* Techniques are used in Lucene. 

[Smaller Representation of Finite State Automata, 2011](http://www.cs.put.poznan.pl/dweiss/site/publications/download/fsacomp.pdf)
* Techniques are used in Lucene. 

# Deep Learning
[Sequence to Sequence Learning with Neural Networks, NIPS 2014](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

[Learning Character-level Representations for Part-of-Speech Tagging, ICML 2014](http://jmlr.org/proceedings/papers/v32/santos14.pdf)
* Character-level tagging. State-of-the-art in Portugese.
