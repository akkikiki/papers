\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{url}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}

\begin{document}

This note explains essential stuff to understand dual decomposition explained in \cite{pfi_dual_decomposition}. 
See \cite{Rush_2012_tutorial} for the full tutorial.

\section{Important Terminologies}
\begin{enumerate}
 \item Subderivative (劣微分), subgradient (劣勾配)
 \item Belief propagation (BP)
 \item Lagrange Relaxation
\end{enumerate}

\section{Main Points}
\begin{enumerate}
 \item Typically simple and efficient \cite{Rush_2012_tutorial}.
 \item Many decoding problems can be decomposed into two or more subproblems \cite{Rush_2012_tutorial}.
 \item ``Dual decomposition [...] is a special case of Lagrangian relaxation (LR).'' \cite{Rush_2012_tutorial}
\end{enumerate}

\begin{equation*}
\begin{aligned}
& \underset{\theta}{\text{maximize}}
& & f(z) + h(y) \\
& \text{subject to}
& & y = z \\
\end{aligned}
\end{equation*}

\section{Dual Decomposition for Parsing}
Cherry-picking importnat points from \cite{Rush_2012_tutorial}.

\section{Why is dual composition/Lagrange relaxation important?}
\begin{enumerate}
 \item Able to include global features rather than local features (e.g., like what Viterbi algorithm does) during the decoding step \cite{pfi_dual_decomposition}. 
\end{enumerate}

\bibliographystyle{plain}
\bibliography{yoshinari_notes_dd}

\end{document}

